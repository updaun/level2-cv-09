{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(심화) 1번 과제 - CNN Visualization (정답).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BUfGvVv6ZHm"
      },
      "source": [
        "# 심화과제 1: CNN Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgVU2ot9BHPt"
      },
      "source": [
        "이번 과제에서는 강의에서 공부했던 다양한 방식의 CNN visualization 방법들을 직접 구현해봅니다. 기본과제3 에서 구현했던 VGG-11을 분석해보며 CNN의 동작과 학습 원리에 대한 좋은 직관을 갖게 되셨으면 좋겠습니다. \n",
        "\n",
        "과제 목표: \n",
        "- 기학습된 모델을 분석하는 방법, 조작하는 방법들에 대해서 익힌다.\n",
        "- 모델 디버깅에 필요한 기본 방법들에 대해서 숙지 한다.\n",
        "- 그레디언트 정보가 포함하고 있는 의미를 이해하고, 이를 분석해 활용할 수 있도록 한다.\n",
        "\n",
        "\n",
        "Special thanks: This PA design is inspired from PA5 of Cornell CS5670 (By Noah Snavely and Abe Davis). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fmD4b_76b71"
      },
      "source": [
        "## **Notes on PyTorch**\n",
        "Many operations are implemented on Tensors (e.g, torch.FloatTensor or torch.LongTensor) in PyTorch, as on ndarray in numpy. One can compute operations like arithmetics, and will get a new Tensor as an output. \n",
        "\n",
        "Also, Variables (e.g, torch.autograd.Variable) wrap tensors and can be used interchangeably as Tensors. Like Tensors, with operation on Variable, the output is a new Variable.\n",
        "\n",
        "Here, Pytorch autograd builds a computational graph with operations on Tensors (involving Variables), with True requires_grad parameter. This indicates to compute gradients of that Tensors w.r.t. the other Tensors 'automatically' with calling backward. \n",
        "\n",
        "With such option, one can calculate the gradient w.r.t. model parameters by calling backward on the loss function we're trying to optimize, allowing us to implement 'gradient descent'. Furthremore, one can call backward to calculate the gradient w.r.t the input. \n",
        "\n",
        "One can access to the gradients (after calling backward) as the .grad attribute of the Tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFgdGpTKSjf3"
      },
      "source": [
        "##**Data Download**\n",
        "해당 문제를 풀기 위해서는 (1) pretrained model 과 (2) 저희가 선별한 inference용도의 data가 따로 필요합니다. 아래의 명령어를 통해 과제 해결에 필요한 파일들을 colab session에 다운로드하고 압축을 해제해주세요.\n",
        "\n",
        "    \n",
        "***Inference를 위해 사용하는 data에는 저작권 문제가 있으니, 과제 해결 이외의 작업에는 절대로 사용하시면 안됩니다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vh1luPPqT6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23e9321-4ffc-4f2f-c957-70b8d5067803"
      },
      "source": [
        "!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BY1Wyq7_pAaf57ZIZ9rcdb2LD29Kgmz5' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1BY1Wyq7_pAaf57ZIZ9rcdb2LD29Kgmz5\" -O files.zip && rm -rf ~/cookies.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-13 09:21:22--  https://docs.google.com/uc?export=download&confirm=1Iyz&id=1BY1Wyq7_pAaf57ZIZ9rcdb2LD29Kgmz5\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.26.138, 74.125.26.101, 74.125.26.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.26.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-3k-docs.googleusercontent.com/docs/securesc/u98jbt1p9v86m7hnpvuuhtq5qc5341ll/400rbhirn8qjcijmtql5r895mljf53q8/1628846475000/09977954582062536659/14364580203194788552Z/1BY1Wyq7_pAaf57ZIZ9rcdb2LD29Kgmz5?e=download [following]\n",
            "--2021-08-13 09:21:22--  https://doc-00-3k-docs.googleusercontent.com/docs/securesc/u98jbt1p9v86m7hnpvuuhtq5qc5341ll/400rbhirn8qjcijmtql5r895mljf53q8/1628846475000/09977954582062536659/14364580203194788552Z/1BY1Wyq7_pAaf57ZIZ9rcdb2LD29Kgmz5?e=download\n",
            "Resolving doc-00-3k-docs.googleusercontent.com (doc-00-3k-docs.googleusercontent.com)... 142.250.98.132, 2607:f8b0:400c:c1a::84\n",
            "Connecting to doc-00-3k-docs.googleusercontent.com (doc-00-3k-docs.googleusercontent.com)|142.250.98.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=ogvg6uadfsrme&continue=https://doc-00-3k-docs.googleusercontent.com/docs/securesc/u98jbt1p9v86m7hnpvuuhtq5qc5341ll/400rbhirn8qjcijmtql5r895mljf53q8/1628846475000/09977954582062536659/14364580203194788552Z/1BY1Wyq7_pAaf57ZIZ9rcdb2LD29Kgmz5?e%3Ddownload&hash=a1rg0e48259o0l206u51ktf00uapm979 [following]\n",
            "--2021-08-13 09:21:22--  https://docs.google.com/nonceSigner?nonce=ogvg6uadfsrme&continue=https://doc-00-3k-docs.googleusercontent.com/docs/securesc/u98jbt1p9v86m7hnpvuuhtq5qc5341ll/400rbhirn8qjcijmtql5r895mljf53q8/1628846475000/09977954582062536659/14364580203194788552Z/1BY1Wyq7_pAaf57ZIZ9rcdb2LD29Kgmz5?e%3Ddownload&hash=a1rg0e48259o0l206u51ktf00uapm979\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.26.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-00-3k-docs.googleusercontent.com/docs/securesc/u98jbt1p9v86m7hnpvuuhtq5qc5341ll/400rbhirn8qjcijmtql5r895mljf53q8/1628846475000/09977954582062536659/14364580203194788552Z/1BY1Wyq7_pAaf57ZIZ9rcdb2LD29Kgmz5?e=download&nonce=ogvg6uadfsrme&user=14364580203194788552Z&hash=hmst5hb9jou5ri8br8orqmago4kp5c27 [following]\n",
            "--2021-08-13 09:21:22--  https://doc-00-3k-docs.googleusercontent.com/docs/securesc/u98jbt1p9v86m7hnpvuuhtq5qc5341ll/400rbhirn8qjcijmtql5r895mljf53q8/1628846475000/09977954582062536659/14364580203194788552Z/1BY1Wyq7_pAaf57ZIZ9rcdb2LD29Kgmz5?e=download&nonce=ogvg6uadfsrme&user=14364580203194788552Z&hash=hmst5hb9jou5ri8br8orqmago4kp5c27\n",
            "Connecting to doc-00-3k-docs.googleusercontent.com (doc-00-3k-docs.googleusercontent.com)|142.250.98.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-zip-compressed]\n",
            "Saving to: ‘files.zip’\n",
            "\n",
            "files.zip               [   <=>              ]  33.83M  63.7MB/s    in 0.5s    \n",
            "\n",
            "2021-08-13 09:21:23 (63.7 MB/s) - ‘files.zip’ saved [35479457]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd7ly0r8qh85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f502d36e-4db4-48cc-c86b-0610703acaf2"
      },
      "source": [
        "!unzip files.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  files.zip\n",
            "replace data/1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/1.jpg              \n",
            "replace data/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: data/0.jpg              \n",
            "  inflating: model.pth               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIlKbJiZ6AK0"
      },
      "source": [
        "## **Utils**\n",
        "과제 해결에 필요한 여러가지 함수들입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jspqRqoSlV6A"
      },
      "source": [
        "### Utils (module)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmHZQQhWlcQn"
      },
      "source": [
        "from os import path\n",
        "import wheel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import functools\n",
        "import urllib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "from google.protobuf import text_format\n",
        "from io import StringIO\n",
        "\n",
        "import PIL.Image\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
        "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
        "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUIuBDT-ljU1"
      },
      "source": [
        "### Utils (image converter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uJklTnnljgU"
      },
      "source": [
        "def image_tensor_to_numpy(tensor_image):\n",
        "  # If this is already a numpy image, just return it\n",
        "  if type(tensor_image) == np.ndarray:\n",
        "    return tensor_image\n",
        "  \n",
        "  # Make sure this is a tensor and not a variable\n",
        "  if type(tensor_image) == Variable:\n",
        "    tensor_image = tensor_image.data\n",
        "  \n",
        "  # Convert to numpy and move to CPU if necessary\n",
        "  np_img = tensor_image.detach().cpu().numpy()\n",
        "  \n",
        "  # If there is no batch dimension, add one\n",
        "  if len(np_img.shape) == 3:\n",
        "    np_img = np_img[np.newaxis, ...]\n",
        "  \n",
        "  # Convert from BxCxHxW (PyTorch convention) to BxHxWxC (OpenCV/numpy convention)\n",
        "  np_img = np_img.transpose(0, 2, 3, 1)\n",
        "  \n",
        "  return np_img\n",
        "\n",
        "def image_numpy_to_tensor(np_image):\n",
        "  if len(np_image.shape) == 3:\n",
        "    np_image = np_image[np.newaxis, ...]\n",
        "  \n",
        "  # Convert from BxHxWxC (OpenCV/numpy) to BxCxHxW (PyTorch)\n",
        "  np_image = np_image.transpose(0, 3, 1, 2)\n",
        "  \n",
        "  tensor_image = torch.from_numpy(np_image).float()\n",
        "  \n",
        "  return tensor_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irK3SJaKlPfO"
      },
      "source": [
        "### Utils (image drawer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5T0h7cWlO3-"
      },
      "source": [
        "def normalize(tensor):\n",
        "  x = tensor - tensor.min()\n",
        "  x = x / (x.max() + 1e-9)\n",
        "  return x\n",
        "\n",
        "def draw_border(image_np, color):\n",
        "  color = np.asarray(color)\n",
        "  s = image_np.shape\n",
        "  image_np = image_np.copy()\n",
        "  image_np[0:5, :, :] = color[np.newaxis, np.newaxis, :]\n",
        "  image_np[:, 0:5, :] = color[np.newaxis, np.newaxis, :]\n",
        "  image_np[s[0]-5:s[0], :, :] = color[np.newaxis, np.newaxis, :]\n",
        "  image_np[:, s[0]-5:s[0], :] = color[np.newaxis, np.newaxis, :]\n",
        "  return image_np\n",
        "\n",
        "def show_image(image, title=None):\n",
        "  np_img = image_tensor_to_numpy(image)\n",
        "  if len(np_img.shape) > 3:\n",
        "    np_img = np_img[0]\n",
        "  np_img = normalize(np_img)\n",
        "  \n",
        "  # plot \n",
        "  np_img = np_img.squeeze()\n",
        "  plt.figure(figsize=(4,4))\n",
        "  plt.imshow(np_img)\n",
        "  plt.axis('off')\n",
        "  if title: plt.title(title)\n",
        "  plt.show()\n",
        "    \n",
        "def show_images(image_list):\n",
        "  for l in image_list:\n",
        "    f, axarr = plt.subplots(1,len(l))\n",
        "    for i,img in enumerate(l):\n",
        "      np_img = image_tensor_to_numpy(img)\n",
        "      if len(np_img.shape) > 3:\n",
        "        np_img = np_img[0]\n",
        "      np_img = normalize(np_img)\n",
        "\n",
        "      np_img = np_img.squeeze()\n",
        "      axarr[i].imshow(np_img)\n",
        "      axarr[i].axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNEnu9hs7F7O"
      },
      "source": [
        "## **Model definition**\n",
        "이전 과제에서 구현하였던 VGG-11의 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLbUPbZOF7sE"
      },
      "source": [
        "class VGG11BackBone(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VGG11BackBone, self).__init__()\n",
        "\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "    # Convolution Feature Extraction Part\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "    self.bn1   = nn.BatchNorm2d(64)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "    self.bn2   = nn.BatchNorm2d(128)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "    self.bn3_1   = nn.BatchNorm2d(256)\n",
        "    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "    self.bn3_2   = nn.BatchNorm2d(256)\n",
        "    self.pool3   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "    self.bn4_1   = nn.BatchNorm2d(512)\n",
        "    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "    self.bn4_2   = nn.BatchNorm2d(512)\n",
        "    self.pool4   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "    self.bn5_1   = nn.BatchNorm2d(512)\n",
        "    self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "    self.bn5_2   = nn.BatchNorm2d(512)\n",
        "    self.pool5   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    x = self.conv3_1(x)\n",
        "    x = self.bn3_1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv3_2(x)\n",
        "    x = self.bn3_2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool3(x)\n",
        "\n",
        "    x = self.conv4_1(x)\n",
        "    x = self.bn4_1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv4_2(x)\n",
        "    x = self.bn4_2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "\n",
        "    x = self.conv5_1(x)\n",
        "    x = self.bn5_1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv5_2(x)\n",
        "    x = self.bn5_2(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class VGG11Classification(nn.Module):\n",
        "  def __init__(self, num_classes = 7):\n",
        "    super(VGG11Classification, self).__init__()\n",
        "\n",
        "    self.backbone = VGG11BackBone()\n",
        "    self.gap      = nn.AdaptiveAvgPool2d(1)\n",
        "    self.fc_out   = nn.Linear(512, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.backbone(x)\n",
        "    x = self.backbone.pool5(x)\n",
        "\n",
        "    x = self.gap(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc_out(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfIz7XFTqj9M"
      },
      "source": [
        "model_root = './model.pth'\n",
        "\n",
        "model = VGG11Classification()\n",
        "model.load_state_dict(torch.load(model_root))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cuswVGX6KKW"
      },
      "source": [
        "## **Dataset**\n",
        "\n",
        "과제 수행을 위한 dataset 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwlMgGlNl3D1"
      },
      "source": [
        "# Dataset\n",
        "class MaskDataset(Dataset):\n",
        "  def __init__(self, data_root, input_size=224, transform=None, shuffle=False):\n",
        "    super(MaskDataset, self).__init__()\n",
        "\n",
        "    self.img_list = sorted(glob(os.path.join(data_root, '*.jpg')))\n",
        "    self.len = len(self.img_list)\n",
        "    self.input_size = input_size\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = self.img_list[index]\n",
        "  \n",
        "    # Image Loading\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img/255.\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H6ZVcWZDKt-"
      },
      "source": [
        "data_root = './data'\n",
        "\n",
        "input_size = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "mask_dataset = MaskDataset(data_root, input_size=input_size, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm4hqh70t8TU"
      },
      "source": [
        "## **Problem 2-1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLzxHyw83_Pi"
      },
      "source": [
        "아래는 학습된 VGG-11를 visualization하는 코드입니다. \n",
        "총 3개의 **TO DO**를 채워보세요.\n",
        "\n",
        "\n",
        "- **TO DO (1)**: 주어진 모듈의 parameter 개수를 return하는 **get_params_num** 코드를 완성해주세요.\n",
        "\n",
        "- **TO DO (2)**: 모델에서 **conv1_filters_data**를 얻는 코드를 완성해주세요.\n",
        "\n",
        "- **TO DO (3)**: Activation을 target layer에 시각화하기 위해 hook function을 register해주세요.\n",
        "\n",
        "- **Hint**: 아래의 코드를 이용해보세요.    \n",
        "(1) [module.register_forward_hook](https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html) \\\n",
        "(2) Tensor.weight.data    \n",
        "(3) torch.size()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ce7Adq97ed3"
      },
      "source": [
        "\n",
        "### **Visualizing Structure**\n",
        "VGG-11의 구조를 각 layer의 weights shape을 통해 확인해봅시다.\n",
        "예를 들어, input channel의 32이고 output channel이 64인 3x3 convolution layer라면 (64, 32, 3, 3)의 weight shape 가집니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2xK_2Ouvmm0"
      },
      "source": [
        "model = VGG11Classification()\n",
        "model.load_state_dict(torch.load(model_root))\n",
        "\n",
        "for name, module in model.named_parameters():\n",
        "  names = name.split(\".\")\n",
        "  if \"backbone\" in name:\n",
        "    n = names[1]\n",
        "  else:\n",
        "    n = names[0]\n",
        "\n",
        "  if \"weight\" in name :\n",
        "    print(f\"** Module {n} **\\n\"\n",
        "          f\"weights: {module.size()}\")\n",
        "  if \"bias\" in name :\n",
        "    print(f\"bias: {module.size()}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wwipjxlx9Xt"
      },
      "source": [
        "아래의 **get_module_params_num**의 **TO DO (1)** 부분에서 각 layer의 parameter의 크기를 *param_num* 변수에 더하여 총 parameter의 개수를 구할 수 있도록 코드를 완성해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI6llx4pDSwU"
      },
      "source": [
        "def get_module_params_num(module):\n",
        "  \"\"\"\n",
        "  Return the parameter number of modules\n",
        "  With parameter in module in shape of (H,W,D), the size of such parameter would be HxWxD\n",
        "\n",
        "  Keyword arguments:\n",
        "  module: the module is composed of several named parameters\n",
        "  \"\"\"\n",
        "  param_num = 0\n",
        "\n",
        "  for _, param in module.named_parameters():\n",
        "    \n",
        "    '''==========================================================='''\n",
        "    '''======================== TO DO (1) ========================'''\n",
        "\n",
        "    param_size = 1\n",
        "    for size in list(param.size()):\n",
        "      param_size *= size\n",
        "    param_num += param_size\n",
        "\n",
        "    '''==========================================================='''\n",
        "    '''======================== TO DO (1) ========================'''\n",
        "\n",
        "  return param_num\n",
        "\n",
        "def get_model_params_num(model):\n",
        "  module_num = 0\n",
        "  for name, module in model._modules.items():\n",
        "    module_num += get_module_params_num(module)\n",
        "  return module_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI93-8tQILA0"
      },
      "source": [
        "num_params = get_model_params_num(model)\n",
        "print(f\"Number of parameters in customed-VGG11: {num_params}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVJslvAR7jVC"
      },
      "source": [
        "### **Visualizing conv1 filters**\n",
        "\n",
        "첫번째 convolution layer는 RGB 형태의 input image을 받기 때문에, 해당 filters의 입력 채널 수가 RGB 채널 수와 같기 때문에, filter를 <!-- 해당 filters를  --> \n",
        "RGB image로 변환하여 시각화할 수 있습니다. (이후 레이어의 다른 filters는 high dimension을 갖고 있기 때문에 첫번째 convolution layer에 비해 시각화가 어렵습니다.)\n",
        "\n",
        "첫번째 convolution layer의 filter를 시각화하기 위하여 해당 layer의 weight를 *conv1_filters_data* 변수에 할당하는 코드를 **TO DO (2)** 부분에 완성해주세요. 전체 모델에서 원하는 레이어의 이름을 **print(model)**을 통해 확인한 다음 해당 layer의 weight를 Tensor로 얻는 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdYgoghkJgWc"
      },
      "source": [
        "def plot_filters(data, title=None):\n",
        "    \"\"\"\n",
        "    Take a Tensor of shape (n, K, height, width) or (n, K, height, width)\n",
        "    and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\n",
        "    \"\"\"\n",
        "    \n",
        "    if data.size(1) > 3:\n",
        "      data = data.view(-1, 1, data.size(2), data.size(3))\n",
        "        \n",
        "    data = image_tensor_to_numpy(data)\n",
        "        \n",
        "    # normalize data for display\n",
        "    data = (data - data.min()) / (data.max() - data.min())\n",
        "    \n",
        "    # force the number of filters to be square\n",
        "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
        "    padding = (((0, n ** 2 - data.shape[0]),\n",
        "               (0, 2), (0, 2))                 # add some space between filters\n",
        "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
        "    data = np.pad(data, padding, mode='constant', constant_values=1)  # pad with ones (white)\n",
        "    \n",
        "    # tile the filters into an image\n",
        "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
        "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
        "    data = data.squeeze()\n",
        "    \n",
        "    # plot it\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.imshow(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K4t-t4Gjkbq"
      },
      "source": [
        "model = VGG11Classification()\n",
        "model.load_state_dict(torch.load(model_root))\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnaw0wCpy-k6"
      },
      "source": [
        "'''==========================================================='''\n",
        "'''======================== TO DO (2) ========================'''\n",
        "\n",
        "conv1_filters_data = model.backbone.conv1.weight.data\n",
        "\n",
        "'''==========================================================='''\n",
        "'''======================== TO DO (2) ========================'''\n",
        "\n",
        "plot_filters(conv1_filters_data, title=\"Filters in conv1 layer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGBDlQh57p_X"
      },
      "source": [
        "### **Visualizing model activations**\n",
        "Pytorch에서는 hook을 통해서 모델의 최종 출력값이 아닌 중간 layer들의 출력값을 얻을 수 있는 편리한 기능을 제공합니다.\n",
        "<!-- 있습니다. -->\n",
        " 모델의 중간의 원하는 부분에 갈고리(hook)를 건 다음, 해당 layer의 출력을 건져올린다고 이해하시면 좋을 것 같습니다.\n",
        "\n",
        "예를 들어, forward hook은 각 module에 대해 forward method가 실행될 때마다 설정한 function이 실행됩니다. 따라서 forward hook에 visualization function을 설정하면 각 layer의 activation을 시각화할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHvKiq0ACR4R"
      },
      "source": [
        "plot_activations = plot_filters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upi3KKwyi6Qw"
      },
      "source": [
        "activation_list = []\n",
        "\n",
        "def show_activations_hook(name, module, input, output):\n",
        "  # conv/relu layer outputs (BxCxHxW)\n",
        "  if output.dim() == 4:\n",
        "    activation_list.append(output)\n",
        "    plot_activations(output, f\"Activations on: {name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9H6idKOCjxV"
      },
      "source": [
        "# Image preparation\n",
        "img = mask_dataset[0]\n",
        "show_image(img)\n",
        "img = Variable(img[np.newaxis, ...])\n",
        "img = img.double()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-WFF1Ki51wO"
      },
      "source": [
        "위에서 선언한 **show_activations_hook** 함수를 이용하여 각 module에 forward hook을 걸어주어 layer의 출력값을 시각화해주세요. \n",
        "\n",
        "- Tip 1. module에 forward hook을 걸어주기 위해서는 module.register_forward_hook을 이용하세요. \\\n",
        "- Tip 2. 함수의 일부 인자가 채워진 상태로 호출하고 싶다면 [functools.partial](https://hamait.tistory.com/823)을 이용해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCF-oq7vJ0Pj"
      },
      "source": [
        "# Re-define the model to clear any previously registered hooks\n",
        "model = VGG11Classification()\n",
        "model.load_state_dict(torch.load(model_root))\n",
        "model.double()\n",
        "\n",
        "# Register the hook on the select set of modules\n",
        "module_list  = [model.backbone.conv1, model.backbone.bn4_1]\n",
        "module_names = [\"conv1\", \"bn4_1\"]\n",
        "\n",
        "# You may use functools.partial to make function already filled with target module name\n",
        "for idx, (name, module) in enumerate(zip(module_names, module_list)):\n",
        "  '''==========================================================='''\n",
        "  '''======================== TO DO (3) ========================'''\n",
        "\n",
        "  hook = functools.partial(show_activations_hook, name)\n",
        "  module.register_forward_hook(hook)\n",
        "\n",
        "  '''==========================================================='''\n",
        "  '''======================== TO DO (3) ========================'''\n",
        "\n",
        "_ = model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Waj7pNRZRHu"
      },
      "source": [
        "np.shape(activation_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI4wNhS8jl4x"
      },
      "source": [
        "#### **Discussion**\n",
        "\n",
        "You may analyze activations in different modules by modifying **module_list**. Check how different activations are between layers near input and near output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOhKfMjouQpy"
      },
      "source": [
        "## **Problem 2-2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs4UsSa-77HR"
      },
      "source": [
        "아래는 saliency map을 시각화하는 코드입니다. Saliency map이란, CNN이 최종 결과를 내리기까지 각 pixel이 기여하고 있는 정도를 시각화하여 나타낸 것으로 activation map이라고도 불립니다. [참고 자료](https://www.geeksforgeeks.org/what-is-saliency-map/). \n",
        "\n",
        "- **TO DO (4)**: Input image에 대한 gradient s_y (the score at index class_idx)를 return하는 함수 **compute_gradient_score**를 완성해주세요.\n",
        "\n",
        "\n",
        "- **Hint**: 원하는 gradient를 얻기 위해 어떤 정보가 back-propagate 되어야 하는지를 생각해보세요! (ds_y / dI)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bYxYbxn8LRO"
      },
      "source": [
        "### **Visualizing saliency**\n",
        "학습된 customed-VGG11을 이용하여 [1]의 Section 3.1을 따라 class saliency maps을 얻을 수 있습니다. 논문의 Section 2를 바탕으로, 이미지의 class score에 대한 gradient를 계산할 수 있습니다. \n",
        "\n",
        "아래의 코드에서 계산하고자 하는 식을 아래와 같습니다.:$${\\partial s_y \\over \\partial I}$$\n",
        "\n",
        "이때 $s_y$는 class $y$에 대한 logit입니다. (fully connected layer를 통과한 다음, 즉 softmax layer를 통과하기 이전의 값).\n",
        "\n",
        "Gradient를 계산한 다음에, 해당 값들을 시각화함으로써 input image에 대한 saliency를 확인할 수 있습니다. 자세한 내용은 아래 논문의 본문을 참고해주세요!\n",
        "\n",
        "[[1] Simonyan et al., Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, ICLR 2014](https://arxiv.org/pdf/1312.6034.pdf)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYVFJQ4hIFij"
      },
      "source": [
        "def compute_gradient_score(scores, image, class_idx):\n",
        "    \"\"\"\n",
        "    Returns the gradient of s_y (the score at index class_idx) w.r.t the input image (data), ds_y / dI. \n",
        "\n",
        "    class_idx에 해당하는 class에 대한 gradient인 s_y를 계산해야 합니다.\n",
        "    전체 class의 개수의 길이를 갖는 scores에서 원하는 index의 score를 s_y로 얻은 다음, 해당 s_y를 back-propagate하여 gradient를 계산하는 코드를 완성해주세요.\n",
        "    \"\"\"\n",
        "    grad = torch.zeros_like(image)\n",
        "\n",
        "    '''==========================================================='''\n",
        "    '''======================== TO DO (4) ========================'''\n",
        "\n",
        "    s_y = scores[class_idx]\n",
        "    s_y.backward()\n",
        "\n",
        "    '''==========================================================='''\n",
        "    '''======================== TO DO (4) ========================'''\n",
        "\n",
        "    grad = image.grad\n",
        "    assert tuple(grad.shape) == (1, 3, 224, 224)\n",
        "\n",
        "    return grad[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQpAA9MSqr8i"
      },
      "source": [
        "def visualize_saliency(image, model):\n",
        "    input = Variable(image.unsqueeze(0), requires_grad=True)\n",
        "    output = model(input)[0]\n",
        "    max_score, max_idx = torch.max(output, 0)\n",
        "\n",
        "    grad = compute_gradient_score(output, input, max_idx)\n",
        "\n",
        "    vis = grad ** 2\n",
        "    vis, _ = torch.max(vis, 0)\n",
        "    \n",
        "    return vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_TuOUSAg5O_"
      },
      "source": [
        "model = VGG11Classification()\n",
        "model.load_state_dict(torch.load(model_root))\n",
        "model.double()\n",
        "\n",
        "input_images = []\n",
        "saliency_maps = []\n",
        "  \n",
        "for _, sample in enumerate(mask_dataset):\n",
        "  saliency_map = visualize_saliency(sample, model)\n",
        "  assert list(saliency_map.shape) == [224, 224]\n",
        "    \n",
        "  saliency_maps.append(saliency_map.unsqueeze(0))\n",
        "  input_images.append(sample)\n",
        "\n",
        "row_list = list(zip(input_images, saliency_maps))\n",
        "show_images(row_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucOucsIIr3Cz"
      },
      "source": [
        "## **Problem 2-3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoAmGsQz-T8C"
      },
      "source": [
        "아래는 Grad-CAM을 시각화하기 위한 코드입니다. 2개의 **TO DO**s를 채워주세요.\n",
        "\n",
        "\n",
        "- **TO DO (5)**: 함수 **vis_gradcam**을 완성해주세요. (1) Layer의 activation을 저장할 function을 hook하고 (2) forward하고 (3) gradients를 저장하기 위해 hook을 register한 다음 (4) 출력에 대한 최댓값에 해당하는 score를 backward해야 합니다. (1) - (3) - (2) - (4)의 순서로 Grad-CAM을 시각화할 수 있습니다.\n",
        "\n",
        "\n",
        "- **TO DO (6)**: 함수 **vis_gradcam**을 완성해주세요. 아래의 텍스트에 설명되어 있는 **Grad-CAM**의 값을 계산한 다음 (2) 원본 이미지의 크기에 맞게 upsampling 해야합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEyMvwpzu0UR"
      },
      "source": [
        "### **Visualizing Grad-CAM**\n",
        "\n",
        "Saliency map을 시각화하기 위하여, [2]의 Section 3.1을 따라 Grad-CAM을 얻을 수 있습니다. \n",
        "\n",
        "첫번째로 $\\alpha_c^k$를 다음 과정으로 계산합니다: $${\\partial y^c \\over \\partial A^k}$$\n",
        "\n",
        "이때 $y^c$는 class $c$에 대한 score이고 $A^k$는 target layer인 $k$-th feature map의 activation입니다.\n",
        "\n",
        "다음으로 $\\alpha_c^k$를 weights로 하여 forward activation maps $A$의 weighted sum을 얻고 $ L_{Grad-CAM}^c $ (i.e, $ ReLU (\\sum_k \\alpha_c^k A^k) $)을 얻기 위하여 ReLU를 통과합니다.\n",
        "\n",
        "\n",
        "[[2] Selvaraju et al., Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, ICCV 2017](https://arxiv.org/abs/1610.02391)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngWh5rqFwRfu"
      },
      "source": [
        "save_feat=[]\n",
        "def hook_feat(module, input, output):\n",
        "  save_feat.append(output)\n",
        "  return output\n",
        "\n",
        "\n",
        "save_grad=[]\n",
        "def hook_grad(grad):\n",
        "  \"\"\"\n",
        "  get a gradient from intermediate layers (dy / dA).\n",
        "  See the .register-hook function for usage.\n",
        "  :return grad: (Variable) gradient dy / dA\n",
        "  \"\"\" \n",
        "  save_grad.append(grad)\n",
        "  return grad\n",
        "\n",
        "\n",
        "def vis_gradcam(vgg, img):\n",
        "  \"\"\"\n",
        "  Imshow the grad_CAM.\n",
        "  :param vgg: VGG11Customed model\n",
        "  :param img: a dog image\n",
        "  output : plt.imshow(grad_CAM)\n",
        "  \"\"\"\n",
        "  vgg.eval()\n",
        "\n",
        "\n",
        "  '''==========================================================='''\n",
        "  '''======================== TO DO (5) ========================'''\n",
        "  # (1) Reister hook for storing layer activation of the target layer (bn5_2 in backbone)\n",
        "  vgg.backbone.bn5_2.register_forward_hook(hook_feat)\n",
        "  \n",
        "  # (2) Forward pass to hook features\n",
        "  img = img.unsqueeze(0)\n",
        "  s = vgg(img)[0]\n",
        "\n",
        "  # (3) Register hook for storing gradients\n",
        "  save_feat[0].register_hook(hook_grad)\n",
        "  \n",
        "  # (4) Backward score\n",
        "  y = torch.argmax(s).item()\n",
        "  s_y = s[y]\n",
        "  s_y.backward()\n",
        "\n",
        "  '''==========================================================='''\n",
        "  '''======================== TO DO (5) ========================'''\n",
        "\n",
        "\n",
        "\n",
        "  # Compute activation at global-average-pooling layer\n",
        "  gap_layer  = torch.nn.AdaptiveAvgPool2d(1)\n",
        "  alpha = gap_layer(save_grad[0][0].squeeze())\n",
        "  A = save_feat[0].squeeze()\n",
        "\n",
        "\n",
        "\n",
        "  '''==========================================================='''\n",
        "  '''======================== TO DO (6) ========================'''\n",
        "  # (1) Compute grad_CAM \n",
        "  # (You may need to use .squeeze() to feed weighted_sum into into relu_layer)\n",
        "  relu_layer = torch.nn.ReLU()\n",
        "\n",
        "  weighted_sum = torch.sum(alpha*A, dim=0)\n",
        "  grad_CAM = relu_layer(weighted_sum)\n",
        "\n",
        "  grad_CAM = grad_CAM.unsqueeze(0)\n",
        "  grad_CAM = grad_CAM.unsqueeze(0)\n",
        "\n",
        "  # (2) Upscale grad_CAM\n",
        "  # (You may use defined upscale_layer)\n",
        "  upscale_layer = torch.nn.Upsample(scale_factor=img.shape[-1]/grad_CAM.shape[-1], mode='bilinear')\n",
        "\n",
        "  grad_CAM = upscale_layer(grad_CAM)\n",
        "  grad_CAM = grad_CAM/torch.max(grad_CAM)\n",
        "\n",
        "\n",
        "  '''==========================================================='''\n",
        "  '''======================== TO DO (6) ========================'''\n",
        "\n",
        "\n",
        "\n",
        "  # Plotting\n",
        "  img_np = image_tensor_to_numpy(img)\n",
        "  if len(img_np.shape) > 3:\n",
        "    img_np = img_np[0]\n",
        "  img_np = normalize(img_np)\n",
        "  \n",
        "  grad_CAM = grad_CAM.squeeze().detach().numpy()\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.imshow(img_np)\n",
        "  plt.imshow(grad_CAM, cmap='jet', alpha = 0.5)\n",
        "  plt.show\n",
        "\n",
        "  return grad_CAM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn0gvwHCDcS2"
      },
      "source": [
        "model = VGG11Classification()\n",
        "model.load_state_dict(torch.load(model_root))\n",
        "model.double()\n",
        "\n",
        "img = mask_dataset[1]\n",
        "res = vis_gradcam(model, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l36bvSvkg5F"
      },
      "source": [
        "#### **Discussion**\n",
        "\n",
        "- 기본 과제 3번의 **segmentation model**의 결과와 이번 과제의 **Grad-CAM**의 결과를 비교해보세요. 어떤 점이 비슷하고 어떤점이 다른지를 위주로 생각해보세요!\n",
        "두 과제는 밀접한 연관을 가지고 기획 되었습니다. GradCAM의 Gradient가 classification to segmentation 에서 어떤 요소와 유사성을 가지고 있는지 수식을 유도해 보기 바랍니다.\n",
        "<!-- 선택 과제 1번의 **segmentation model**의 결과와 **Grad-CAM**의 결과를 비교해보세요. 어떤 점이 비슷하고 어떤점이 다른지를 위주로 생각해보세요!  -->\n",
        "\n",
        "- 위 Problem 2-2에서 언급한 Gradient가 class에 대한 saliency 정보를 포함하게 되는 이유에 대해서 고찰해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyDxev82KS0y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}