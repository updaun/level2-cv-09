{"cells":[{"cell_type":"markdown","metadata":{"id":"FGYv5jjqps3z"},"source":["# **기본과제 5 - CLIP 모델을 통한 멀티모달(Multi-modal) 모델의 다양한 활용**\n","    \n","    과제 목표\n","        1. CLIP 모델 Architecture의 정성적 이해\n","        2. 학습된 CLIP 멀티모달 모델을 활용한 Image Classification \n","        3. 학습된 CLIP 멀티모달 모델을 활용한 Image-Text Matching Task\n","        4. 학습된 CLIP 멀티모달 모델을 응용한 Text2Image 모델 학습 및 활용\n","        \n","    CLIP [1] 이란?\n","    : OpenAI에서 만든 멀티모달 모델을 지칭하는 단어로, \"Contrastive Language-Image Pre-Training\"의 약자이기도 합니다.\n","      직역해보자면 \"언어와 이미지 데이터를 Constrastive Learning으로 사전 학습 시키기\" 라고 할 수 있는데,\n","      이름에서 알수있듯 \"이미지\"와 \"텍스트\" 두 가지 다른 타입의 데이터로 학습을 하기 때문에 멀티모달: 모델로 분류됩니다.\n","\n","      CLIP을 활용하면 이미지와 텍스트로부터 유용한 공통 특징 (feature) 공간의 feature를 추출할 수 있고 이를 활용하여 다양한 task를 수행할 수 있습니다.\n","      [1] Radford et al., Learning Transferable Visual Models From Natural Language Supervision, International Conference on Machine Learning (ICML), 2021."]},{"cell_type":"markdown","metadata":{"id":"4i3qfBMHp-JW"},"source":["## **Multi-Modal Model인 CLIP의 전체적 학습 과정은?**\n","<br>\n","\n","![image](https://drive.google.com/uc?id=13nymBlxVY2hoUw8t1_cfp3p7dv5baFme)\n","\n","    논문 Figure.1) CLIP 모델의 학습 과정 도식도\n","　  **Fig.1 - (1)** <br>\n","　　　CLIP에서 수행한 Constrastive Learning을 보여줍니다. <br>\n","　　　Image Encoder에서 나온 feature vector들이 I<sub>1</sub>, I<sub>2</sub>, I<sub>3</sub>, ... <br>\n","　　　Image Encoder에서 나온 feature vector들은 T<sub>1</sub>, T<sub>2</sub>, T<sub>3</sub>, ... <br>\n","　　　이며, 각각의 벡터들의 내적 값을 구한 것을 행렬로 표현하고 있습니다.  <br> 　　　대각성분은(하늘색) 이미지와 텍스트의 의미가 맞는 sample(Positive sample)을 의미하며 <br>\n","　　　그 외의 성분은 모두 다른 의미를 가지고 Negative sample이라고 합니다.\n","\n","　　**Fig.1 - (2)** <br>\n","　　　Text Encoder에 각 label text dataset을 넣어서 각각의 feature vector를 뽑아냅니다. <br>\n","　　　마치 첫번째 그림의 a cat staring front, a cat running backward 문장들이 바로 ladel text dataset의 예시로 볼 수 있습니다. <br>\n","\n","　　**Fig.1 - (3)** <br>\n","　　　Image Encoder에 원하는 input 이미지를 넣고, Text encoder에서 나온 각각의 feature vector와 비교하여 similarity를 구합니다. <br>\n","　　　이 중 가장 높은 silmilarity를 보이는 문장이 바로 input 이미지와 가장 유사한 의미의 문장이라고 볼 수 있습니다. <br>\n","　　　zero-shot prediction 이라고 표현한 이유는, 추가적 학습없이 임의의 image와 text를 비교하는 task이기 때문입니다."]},{"cell_type":"markdown","metadata":{"id":"JXBnz2XJ7IYR"},"source":["#### Contrastive Learning 이란?\n","    \n","    위 논문의 Fig.1은 Contrastive Pre-training으로부터 시작합니다. Contrastive Learning이란 무엇일까요?\n","    단어의 의미에서 유추할 수 있듯 서로 다른 경로에서 온 데이터 Feature를 \n","    서로 대조하여 그 '차이'를 학습하는 것을 Contrastive Learning이라 합니다.\n","    아래의 그림은 강의에서 다룬 내용인데요, Contrastive learning의 한 학습 예시를 보여줍니다.\n","     \n","<br>\n","\n","![image](https://drive.google.com/uc?id=1arkUckpZxR8oL1aQeQItHxWv1uSm-q9G)\n","\n","<br>\n","    \n","    위의 그림은 Multi-Modal 학습을 위한 Metric Learning의 과정을 보여주고 있습니다.\n","\n","    Joint Embedding 공간에서 Matching 되는 데이터끼리는 서로 가깝게,\n","    Non-Matching 되는 데이터끼리는 서로 멀어지게끔 학습을 하게 됩니다.\n","\n","    예시 그림에는 동일한 토끼 그림 2개와 각각 짝을 이루는 텍스트(어구 혹은 문장)가 주어졌습니다.\n","\n","    이 때 model은 주어진 각 그림과 텍스트로부터 나온 feature vector를 비교하여\n","    서로 동일한 의미를 지니는 벡터끼리는 Joint Embedding 공간에서 거리를 더욱 가깝게,\n","    서로 다른 의미를 가지면 거리를 더욱 멀어지게 학습하게 됩니다.\n","    \n","    따라서 Contrastive Learning에서는 '다르다' 혹은 '유사하다' 라는 기준을 어떻게 잡을지(=labeling 이슈), \n","    비교를 위한 metric은 무엇으로 정의할지(예를 들어 Euclidean distance, Cosine similarity 등등) 등이 중요 사안입니다.\n","    "]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Y8OVLI6Mprip"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'CLIP'...\n","remote: Enumerating objects: 195, done.\u001b[K\n","remote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 195 (delta 12), reused 22 (delta 9), pack-reused 168\u001b[K\n","Receiving objects: 100% (195/195), 8.91 MiB | 12.67 MiB/s, done.\n","Resolving deltas: 100% (94/94), done.\n"]}],"source":["# CLIP의 git repo를 clone하고 필요한 pakage들을 설치하겠습니다.\n","!git clone https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WBMa8fStprl-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-1j3fmxek\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-1j3fmxek\n","  Resolved https://github.com/openai/CLIP.git to commit 40f5484c1c74edd83cb9cf687c6ab92b28d8b656\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting regex\n","  Downloading regex-2022.3.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.9/764.9 KB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from clip==1.0) (4.62.3)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from clip==1.0) (1.10.2)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from clip==1.0) (0.11.3)\n","Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.5)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->clip==1.0) (3.10.0.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->clip==1.0) (8.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.22.3)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369221 sha256=5069599723e353913690864869879295cdcd9818d097c9de97b7c6bbd3d67fd4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ku9r8u4q/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n","Successfully built clip\n","Installing collected packages: regex, ftfy, clip\n","Successfully installed clip-1.0 ftfy-6.1.1 regex-2022.3.15\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n","You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mcMnVgEtpro9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ftfy in /opt/conda/lib/python3.8/site-packages (6.1.1)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (2022.3.15)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (4.62.3)\n","Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.8/site-packages (from ftfy) (0.2.5)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n","You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install ftfy regex tqdm"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_n8flsfQprr3"},"outputs":[],"source":["import os\n","import PIL\n","import clip\n","import torch\n","import random\n","import imageio\n","import requests\n","import numpy as np\n","from io import BytesIO\n","from tqdm import tqdm\n","from glob import glob\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import torchvision\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import CIFAR100\n","import torchvision.transforms.functional as TF\n","from torch.nn import CosineSimilarity as CosSim"]},{"cell_type":"markdown","metadata":{"id":"oekysD1asNag"},"source":["## **CLIP 모델 활용 첫번째. 모델 불러오기 및 간단한 테스트**\n","    CLIP 모델을 활용하여 다양한 image와 text로 부터 성능을 테스트해보고, feature vector를 추출해보겠습니다. 코드는 CLIP 공식 github을 참고하였습니다.\n","    "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"2uRH8GQ4prux"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|███████████████████████████████████████| 338M/338M [00:05<00:00, 63.0MiB/s]\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# GPU 메모리 약 1.5 GB 필요 --> 만일 부족하다면 clip.available_models() 명령어를 통해 가지고 오는 모델을 바꿀 수 있습니다\n","model, preprocess = clip.load(\"ViT-B/32\", device=device) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhCFhCwiprxq"},"outputs":[],"source":["# Test 이미지를 불러오기\n","image_path = \"./CLIP/CLIP.png\" \n","plt.figure(figsize=(20, 20))  \n","plt.imshow(Image.open(image_path)) \n","plt.show()  "]},{"cell_type":"markdown","metadata":{"id":"FzYF0gAbsU1X"},"source":["#### CLIP 논문의 Figure.1 일부분이 Crop된 이미지가 출력된 것을 보실 수 있습니다.\n","#### 이제 이 그림을 가지고 Text 매칭을 시켜보겠습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QqQ1-78vpr08"},"outputs":[],"source":["# image encoder에 넣을 수 있도록 전처리를 합니다.\n","# preprocess 함수는 (224, 224)로 Resize를 해주고 Normalization을 해줍니다.\n","image = preprocess(Image.open(image_path)).unsqueeze(0).to(device) "]},{"cell_type":"markdown","metadata":{"id":"QjH62VSVseQp"},"source":["##### 어떤 단어가 위 Figure1을 표현할 수 있을까요? 우선 a diagram, a dog, a cat 이라는 단어들로 테스트를 해보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZu86VlVpr4B"},"outputs":[],"source":["# Test 하고자 하는 임의의 문장 or 어구\n","text_dataset = [\"a diagram\", \"a dog\", \"a cat\"]\n","\n","# CLIP 모델 안에는 tokenizer가 내장되어있습니다. 각 단어를 숫자로 변환해줍니다.\n","text = clip.tokenize(text_dataset).to(device) "]},{"cell_type":"markdown","metadata":{"id":"1QDv8HRTs9DK"},"source":["#### 각 image와 text의 유사도 구해보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlpoUlZupr7A"},"outputs":[],"source":["with torch.no_grad():\n","    \n","    # 모델에 image와 text 둘 다 input으로 넣고, 각 text와 image와의 유사도를 구합니다. 값이 클수록 유사합니다.\n","    logits_per_image, _ = model(image, text)\n","    \n","    # 확률값으로 표현하기 위해 softmax를 값을 구합니다.\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy().flatten()\n","\n","print(\"- Text와 image의 유사도 값 -\")\n","for idx in range(len(text_dataset)):\n","    print(\"  \"+text_dataset[idx] + \":\", logits_per_image.cpu().numpy().flatten()[idx])\n","    \n","print(\"\\n- 각 Text가 image와 일치할 확률 -\")\n","for idx in range(len(text_dataset)):\n","    print(\"  \"+text_dataset[idx] + \":\", round(probs[idx]*100, 3) , \"%\")"]},{"cell_type":"markdown","metadata":{"id":"fWSFG0DKtE9j"},"source":["    　input 이미지는 dog나 cat보다 diagram과 의미상 더 유사하다고 볼 수 있는데요, 확률값도 이를 잘 반영하고 있습니다.\n","    　이처럼 CLIP을 활용하면 text와 image 간의 유사한 정도를 쉽게 구할 수 있습니다.  \n","    　이제부터는 다양한 이미지와 텍스트에 대해서 CLIP을 활용하여 유사도를 구해보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwpVAxEkpr98"},"outputs":[],"source":["image_urls = ['https://url.kr/qcutfr',\n","              'https://url.kr/m745rv',\n","              'https://url.kr/i3u6yd'] # 테스트 이미지 링크\n","texts = [[\"a photo of 1 objects\", \"a photo of 3 objects\", \"a photo of 4 objects\", \"a photo of 5 objects\", \"a photo of 10 objects\"],\n","         [\"a photo of a german shepherd dog\", \"a photo of a collie\", \"a photo of a border collie\", \"a photo of a siberian husky\", \"a photo of a rottweiler\"],\n","         [\"a zoomed in photo of a \\\"red and white triangle with exclamation mark warning\\\" traffic sign\", \n","          \"a zoomed in photo of a \\\"red and white triangle with black right curve approaching warning\\\" traffic sign\", \n","          \"a zoomed in photo of a \\\"red and white triangle car skidding / slipping warning\\\" traffic sign\", \n","          \"a zoomed in photo of a \\\"red and white triangle rough / bumpy road warning\\\" traffic sign\", \n","          \"a zoomed in photo of a \\\"red and white triangle with black left curve approaching warning\\\" traffic sign\"]] # 테스트 텍스트 데이터\n","labels = [2, 3, 0]\n","\n","# 한번의 for loop에서 하나의 이미지, 텍스트 데이터에 대해서 유사도 및 확률값을 구합니다.\n","for idx, path in enumerate(image_urls):\n","    img = Image.open(BytesIO(requests.get(path).content))\n","  \n","    plt.figure(figsize=(4,4))\n","    plt.title(f\"Test Image {idx+1}\")\n","    plt.imshow(img)\n","    plt.show()\n","    \n","    # 이미지와 텍스트를 벡터로 만들고 유사도와 확률값을 구합니다.\n","    image = preprocess(img).unsqueeze(0).to(device) \n","    text_dataset = texts[idx]\n","    text = clip.tokenize(text_dataset).to(device) \n","    with torch.no_grad():\n","        logits_per_image, _ = model(image, text)\n","        probs = logits_per_image.softmax(dim=-1).cpu().numpy().flatten()\n","\n","    print(\"- Text와 image의 유사도 값 -\")\n","    for i in range(len(text_dataset)):\n","        print(\"  \"+text_dataset[i] + \":\", logits_per_image.cpu().numpy().flatten()[i])\n","\n","    print(\"\\n- 각 Text가 image와 일치할 확률 -\")\n","    for i in range(len(text_dataset)):\n","        if i == labels[idx]:\n","            print(\"  \"+text_dataset[i] + \":\", round(probs[i]*100, 3) , \"% (실제 정답)\")\n","        else:\n","            print(\"  \"+text_dataset[i] + \":\", round(probs[i]*100, 3) , \"%\")"]},{"cell_type":"markdown","metadata":{"id":"outbFBJ0HpXm"},"source":["    위 결과를 보면 사전 학습 전혀없이 \"물체가 몇 개가 있는지\", \"이 그림이 시베리안 허스키인지\" \n","    혹은 \"표지판의 의미가 무엇인지\" 등을 정답을 가장 높은 확률값으로 추정하는 것을 확인할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"9ksUt2okHpMg"},"source":["#### image encoder와 text encoder를 활용하여 각 image와 text에 대응하는 feature 벡터 만들기\n","\n","    앞선 과정은, model에 image와 text를 한번에 input으로 넣고 곧바로 유사도를 얻었습니다.\n","    그렇지만 때때로 input 데이터의 feature를 추출해야할 때도 있습니다.\n","    \n","    이제부터는 feature vector를 각 input에 대해 얻고 그로부터 유사도를 구해보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B526xDAOIZMD"},"outputs":[],"source":["# 앞서 활용한 Test 이미지와 텍스트를 그대로 활용하겠습니다. \n","image_path = \"./CLIP/CLIP.png\" \n","image = preprocess(Image.open(image_path)).unsqueeze(0).to(device) \n","text_dataset = [\"a diagram\", \"a dog\", \"a cat\"]\n","text = clip.tokenize(text_dataset).to(device) \n","\n","# model의 image encoder와 text encoder에 각각의 데이터를 넣어줍니다.\n","with torch.no_grad():\n","    image_features = model.encode_image(image) # 이미지 feature 추출\n","    text_features = model.encode_text(text)    # 텍스트 feature 추출\n","print(image_features.shape, text_features.shape) "]},{"cell_type":"markdown","metadata":{"id":"4t3uOkmwIZEz"},"source":["##### 각 이미지와 text 데이터가 512차원의 feature 벡터로 변환되었음을 확인할 수 있습니다.\n","##### 이제 feature 벡터에서 유사도를 구하고, 앞서 구했었던 유사도와 정말로 동일한 값을 주는지 확인해보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUe7GXtDIY3r"},"outputs":[],"source":["# feature vector로부터 유사도 계산을 위해 Cosine Similarity 함수를 활용하겠습니다. \n","cos_sim = CosSim(dim=1)\n","logits = cos_sim(image_features, text_features)*100 # similarity의 최대값을 100점처럼 표현하기 위해 편의상 CLIP에서는 100을 곱합니다. \n","probs = logits.softmax(dim=-1).cpu().numpy().flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVJicUzvIYwj"},"outputs":[],"source":["print(\"- Text와 image의 유사도 값 -\")\n","for idx in range(len(text_dataset)):\n","    print(\"  \"+text_dataset[idx] + \":\", logits.cpu().numpy().flatten()[idx])\n","    \n","print(\"\\n- 각 Text가 image와 일치할 확률 -\")\n","for idx in range(len(text_dataset)):\n","    print(\"  \"+text_dataset[idx] + \":\", round(probs[idx]*100, 3) , \"%\")"]},{"cell_type":"markdown","metadata":{"id":"i6uLNF6CIz1N"},"source":["##### 앞서 model에 이미지와 텍스트를 한번에 넣어 구한 값(아래)과 비교하여 어떤가요? \n","    - Text와 image의 유사도 값 -\n","      a diagram: 25.55\n","      a dog: 20.08\n","      a cat: 19.75\n","\n","    - 각 Text가 image와 일치할 확률 -\n","      a diagram: 99.268 %\n","      a dog: 0.418 %\n","      a cat: 0.302 %\n","거의 동일한 값임을 확인할 수 있습니다.\n","\n","이제 자신만의 이미지와 텍스트 데이터로 Feature를 추출하고, 유사도를 구하여 확률값을 출력해보겠습니다. <br>\n","어렵다고 생각되는 부분은 앞의 코드를 참고해보세요."]},{"cell_type":"markdown","metadata":{"id":"FkDA0ePLY77I"},"source":["#### **TO-DO Codes - 1** ####"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_G0lj-QIzk9"},"outputs":[],"source":["# 테스트 해보고 싶은 이미지의 인터넷 링크 주소를 입력하세요.\n","image_url = \"      \"\n","img = Image.open(BytesIO(requests.get(image_url).content))\n","plt.imshow(img)\n","plt.show()\n","image = preprocess(img).unsqueeze(0).to(device) \n","\n","# 테스트 해보고 싶은 어구 또는 문장을 입력하세요.\n","# NOTE: CLIP 모델 encoder는 입력받는 문장의 단어 개수가 77개가 넘어서는 안됩니다.\n","text_dataset = [\"    \", \"    \", \"    \", \"     \", \"    \"]\n","\n","# CLIP에서 제공하는 토크나이저를 사용(위 코드 참조)합니다.\n","text =     \n","\n","# model의 image encoder와 text encoder에 각각의 데이터를 넣어줍니다.\n","with torch.no_grad():\n","    image_features =      # 이미지 feature 추출\n","    text_features =       # 텍스트 feature 추출\n","\n","print(image_features.shape, text_features.shape) "]},{"cell_type":"markdown","metadata":{"id":"Im3KmMtyIzcL"},"source":["#### **TO-DO Codes - 2** ####"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Apagdo3JIYpZ"},"outputs":[],"source":["# Cosine Similarity 함수를 활용하여 Similarity를 구합니다. \n","logits =     # similarity의 최대값을 100점처럼 표현하기 위해 편의상 CLIP에서는 100을 곱합니다. (위 코드 참조)\n","probs =  \n","\n","print(\"- Text와 image의 유사도 값 -\")\n","for idx in range(len(text_dataset)):\n","    print(\"  \"+text_dataset[idx] + \":\", logits.cpu().numpy().flatten()[idx])\n","    \n","print(\"\\n- 각 Text가 image와 일치할 확률 -\")\n","for idx in range(len(text_dataset)):\n","    print(\"  \"+text_dataset[idx] + \":\", round(probs[idx]*100, 3) , \"%\")"]},{"cell_type":"markdown","metadata":{"id":"-VVx9hj8ZmV7"},"source":["    자신이 생각한 정답과 이미지가 잘 일치하나요? 다양한 이미지로 테스트를 해보면서 모델 성능을 확인해보세요 :) "]},{"cell_type":"markdown","metadata":{"id":"YO8sCP5AKzzj"},"source":["## **CLIP 모델 활용 두번째. Zero-shot Image Classification**    "]},{"cell_type":"markdown","metadata":{"id":"AtaBnWJ1KyYj"},"source":["##### CIFAR-100 Dataset을 Zero-shot으로 Image Classification을 해보겠습니다."]},{"cell_type":"markdown","metadata":{"id":"_2ii0hyyZ4gH"},"source":["#### **TO-DO Codes - 3** #### "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wliJs715KyMb"},"outputs":[],"source":["# CIFAR100 dataset을 다운로드 받습니다.\n","cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n","\n","# CIFAR 100 모든 classes의 어구를 만들고 토큰화합니다.\n","text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n","\n","# 임의의 input 이미지를 선정.\n","image_index =       #  image_index 값은 0~9999 까지 입력이 가능합니다. 자유롭게 선택해보세요.\n","image, _ = cifar100[image_index]\n","plt.imshow(image)\n","plt.show()\n","\n","# 이미지 전처리 및 feature 추출\n","image_inputs =       # CLIP 모델의 전처리 모듈 사용 (위 코드 참조) \n","\n","with torch.no_grad():\n","    image_features =          # 이미지 feature 추출\n","    text_features =           # 텍스트 feature 추출\n","\n","# Cosine Silmilarity 계산\n","similarity =       # similarity의 최대값을 100점처럼 표현하기 위해 편의상 CLIP에서는 100을 곱합니다. (위 코드 참조)\n","\n","\n","K =     \n","values, indices =          # softmax 함수로 가장 높은 K개의 확률값 구하기 \n","print(\"\\nTop predictions:\\n\") \n","for value, index in zip(values, indices):\n","    print(f\"{cifar100.classes[index]:>16s}: {100*value.item():.2f}%\") "]},{"cell_type":"markdown","metadata":{"id":"GMEiH0xsqYgl"},"source":["    위의 image_index=3000 변수 값을 바꿔가며 CLIP 모델이 어떤 이미지를 잘 분류하고 어떤 이미지에는 취약한지 확인해보세요."]},{"cell_type":"markdown","metadata":{"id":"67WZAj6kKx5g"},"source":["#### 스무고개 게임 (옵션)\n","    CLIP 모델을 활용한 간단한 스무고개 게임을 만들어보는 것은 어떨까요? \n","    \n","    예를 들어 자동차 사진이 하나 주어진다면, \n","    처음에는 노이즈가 많이 들어간 자동차 사진으로 사람이 추측해봅니다. 추측한 단어를 입력한 후 확률값을 확인하여\n","    만일 정답(자동차)의 label이 50%를 넘지 못한다면 이번엔 노이즈가 적게 들어간 사진을 제시합니다.\n","    다시 추측하여 단어를 입력 후 확률값을 확인해봅니다.\n","    \n","    이런식으로 사람이 맞출 때까지 사진을 더 선명하게 바꿔가며 스무고개 게임을 만들어, \n","    친구들과 서로 준비한 사진으로 스무고개 내기를 해보면 어떨까요? 재미있으면서도 유익할 것 같습니다. ^^  "]},{"cell_type":"markdown","metadata":{"id":"cBCniS31KxyM"},"source":["## **CLIP 모델 활용 세번째. Applications with using CLIP**    "]},{"cell_type":"markdown","metadata":{"id":"K25VYhtjKxsP"},"source":["    CLIP은 활용범위가 넓어 현재까지 다양한 응용 연구들이 나왔습니다.\n","    \n","    아래 소개하는 모델은 Text를 입력하면 그에 맞는 Image를 생성하는 CLIP의 Feature를 이용한 Text2Image 모델입니다.\n","    직접 원하는 Text를 입력하여 어떤 이미지가 생성되는지 테스트해보세요.\n","\n","    NOTE: CLIP으로부터 추출한 Feature를 이용하여 생성 모델을 처음부터 학습하므로 다소 시간이 걸립니다.\n","    　　　10~15분정도 실행시켜놓고 잠시 쉬는 시간을 가져보는 것도 좋을 것 같습니다 :) \n","\n","    참고 GitHub 코드 링크: https://github.com/tg-bomze/collection-of-notebooks/blob/master/Text2Image.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JwIoaxm2Kxlc"},"outputs":[],"source":["#@title 필요 패키지 및 모델 정의 (왼쪽 화살표를 눌러 실행해주세요)\n","try: \n","  !pip3 install googletrans==3.1.0a0\n","  from googletrans import Translator, constants\n","  from pprint import pprint\n","  translator = Translator()\n","except: pass\n","\n","from IPython import display\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","import glob\n","from google.colab import output\n","import torch.nn as nn\n","\n","!mkdir frames\n","import moviepy.editor as mpy\n","from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n","from google.colab import files\n","import warnings\n","from IPython.display import clear_output\n","\n","def displ(img, pre_scaled=True):\n","  img = np.array(img)[:,:,:]\n","  img = np.transpose(img, (1, 2, 0))\n","  if not pre_scaled:\n","    img = scale(img, 48*4, 32*4)\n","  imageio.imwrite('result.png', np.array(img))\n","  file_path = '/content/CLIP/frames/{}.png'.format(str(len(os.listdir('/content/CLIP/frames'))).zfill(5))\n","  imageio.imwrite(file_path, np.array(img))\n","  return display.Image('result.png')\n","\n","def card_padded(im, to_pad=3):\n","  return np.pad(np.pad(np.pad(im, [[1,1], [1,1], [0,0]],constant_values=0), [[2,2], [2,2], [0,0]],constant_values=1),\n","            [[to_pad,to_pad], [to_pad,to_pad], [0,0]],constant_values=0)\n","\n","class SineLayer(nn.Module):\n","    def __init__(self, in_features, out_features, bias=True,\n","                 is_first=False, omega_0=30):\n","        super().__init__()\n","        self.omega_0 = omega_0\n","        self.is_first = is_first\n","        \n","        self.in_features = in_features\n","        self.linear = nn.Linear(in_features, out_features, bias=bias)\n","        \n","        self.init_weights()\n","    \n","    def init_weights(self):\n","        with torch.no_grad():\n","            if self.is_first:\n","                self.linear.weight.uniform_(-1 / self.in_features, \n","                                             1 / self.in_features)      \n","            else:\n","                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n","                                             np.sqrt(6 / self.in_features) / self.omega_0)\n","        \n","    def forward(self, input):\n","        return torch.sin(self.omega_0 * self.linear(input))\n","    \n","    def forward_with_intermediate(self, input): \n","        intermediate = self.omega_0 * self.linear(input)\n","        return torch.sin(intermediate), intermediate\n","    \n","class Siren(nn.Module):\n","    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=True, \n","                 first_omega_0=30, hidden_omega_0=30.):\n","        super().__init__()\n","        \n","        self.net = []\n","        self.net.append(SineLayer(in_features, hidden_features, \n","                                  is_first=True, omega_0=first_omega_0))\n","\n","        for i in range(hidden_layers):\n","            self.net.append(SineLayer(hidden_features, hidden_features, \n","                                      is_first=False, omega_0=hidden_omega_0))\n","\n","        if outermost_linear:\n","            final_linear = nn.Linear(hidden_features, out_features)\n","            \n","            with torch.no_grad():\n","                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n","                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n","                \n","            self.net.append(final_linear)\n","        else:\n","            self.net.append(SineLayer(hidden_features, out_features, \n","                                      is_first=False, omega_0=hidden_omega_0))\n","        \n","        self.net = nn.Sequential(*self.net)\n","    \n","    def forward(self, coords):\n","        coords = coords.clone().detach().requires_grad_(True)\n","        output = self.net(coords.cuda())\n","        return output.view(1, sideX, sideY, 3).permute(0, 3, 1, 2)\n","\n","    def forward_with_activations(self, coords, retain_grad=False):\n","        '''Returns not only model output, but also intermediate activations.\n","        Only used for visualizing activations later!'''\n","        activations = OrderedDict()\n","\n","        activation_count = 0\n","        x = coords.clone().detach().requires_grad_(True)\n","        activations['input'] = x\n","        for i, layer in enumerate(self.net):\n","            if isinstance(layer, SineLayer):\n","                x, intermed = layer.forward_with_intermediate(x)\n","                \n","                if retain_grad:\n","                    x.retain_grad()\n","                    intermed.retain_grad()\n","                    \n","                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n","                activation_count += 1\n","            else: \n","                x = layer(x)\n","                \n","                if retain_grad:\n","                    x.retain_grad()\n","                    \n","            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n","            activation_count += 1\n","\n","        return activations\n","\n","def get_mgrid(sidelen, dim=2):\n","    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n","    sidelen: int\n","    dim: int'''\n","    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n","    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n","    mgrid = mgrid.reshape(-1, dim)\n","    return mgrid\n","\n","def checkin(loss):\n","  with torch.no_grad():\n","    al = nom(model(get_mgrid(sideX)).cpu()).numpy()\n","  for allls in al:\n","    displ(allls)\n","    clear_output()\n","    pic_num = str(len(os.listdir('/content/CLIP/frames')))\n","    print(f'Picture number {pic_num}\\n')\n","    if int(pic_num) == 1:\n","      print(\"\\n첫번째 이미지: GAN의 첫 이미지는 매우 Noisy 한 것이 일반적입니다.\")\n","    display.display(display.Image('result.png'))\n","    print('\\n다음 이미지 생성 중.. (시간이 다소 걸립니다)')\n","\n","def ascend_txt():\n","  out = model(get_mgrid(sideX))\n","\n","  cutn = 64\n","  p_s = []\n","  for ch in range(cutn):\n","    size = torch.randint(int(.5*sideX), int(.98*sideX), ())\n","    offsetx = torch.randint(0, sideX - size, ())\n","    offsety = torch.randint(0, sideX - size, ())\n","    apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n","    apper = torch.nn.functional.interpolate(apper, (224,224), mode='bilinear')\n","    p_s.append(nom(apper))\n","  into = torch.cat(p_s, 0)\n","\n","  iii = perceptor.encode_image(into)\n","  t = perceptor.encode_text(tx.cuda())\n","  return -100*torch.cosine_similarity(t, iii, dim=-1).mean()\n","\n","def train():\n","  loss = ascend_txt()\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  if itt % frames_frequency == 0:\n","    checkin(loss)\n","\n","# 원하는 CLIP 모델 로드 \n","perceptor, _ = clip.load('ViT-B/32')\n","nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n","\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline\n","%cd /content/CLIP/\n","%mkdir frames\n","\n","clear_output()\n","!nvidia-smi -L\n","print('\\nDone!')"]},{"cell_type":"markdown","metadata":{"id":"0IECv9JKmQEd"},"source":["##### 세부 파라미터 설정\n","    아래의 text 변수에 생성 이미지를 묘사하는 텍스트를 입력\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hC-1aSWQmPZr"},"outputs":[],"source":["text = 'a white house in a green forest' \n","\n","# 영어 이외의 text를 입력에 대비 \n","translation = translator.translate(text)\n","prompt = translation.text\n","tx = clip.tokenize(prompt)"]},{"cell_type":"markdown","metadata":{"id":"mEga3nk8mzLe"},"source":["    비록 CLIP의 feature를 활용하지만 생성 모델은 일반적으로 학습에 다소 시간이 걸립니다.\n","    이미지 해상도(resolution)과 출력빈도(frame_frequency)를 조절해보세요. 128로 할 경우 어느정도 텍스트와 유사한 이미지를 얻기 위해 약 5~10분 정도 소요됩니다.\n","    \n","    [변수명]\n","    resolution: 이미지 가로 세로 해상도. 128은 크기가 작지만 결과가 빠르게 나옵니다.\n","    frame_frequency: 중간 결과 출력 빈도 (GradientDecent를 통한 모델 업데이트를 몇 번 한 후 출력할지) \n","    stop_iter: 중간 결과 총 출력 횟수 (stop_iter 만큼 이미지 생성 후 학습 중단. 일반적으로 20~40 정도에서 어느정도 원하는 이미지가 출력됩니다)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHnr_fFgn0_S"},"outputs":[],"source":["resolution = 256     # 예시: 128, 256, 512\n","frames_frequency = 5 # 예시: 10, 50, 100 \n","stop_iter = 20       # 예시: 10, 20, 50\n","\n","sideX, sideY, channels = resolution, resolution, 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBnreijHKxex"},"outputs":[],"source":["!rm -rf /content/CLIP/frames/*.*\n","model = Siren(in_features=2, out_features=3, hidden_features=256, \n","                  hidden_layers=16, outermost_linear=False).cuda()\n","optimizer = torch.optim.Adam(model.parameters(), .00001)\n","\n","for itt in range(stop_iter*frames_frequency):\n","  train()"]},{"cell_type":"markdown","metadata":{"id":"8mJPGfAfjjs5"},"source":["#### **비디오 생성** \n","    지금까지 생성한 이미지들로 비디오를 생성합니다.(이미지 10개=1초)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uM3qlEVCEXzE"},"outputs":[],"source":["frames = []\n","img = os.listdir(\"/content/CLIP/frames\")\n","img.sort()\n","for i in img:\n","  frames.append(imageio.imread(\"/content/CLIP/frames/\"+i))\n","frames = np.array(frames)\n","imageio.mimsave('/content/CLIP/video.mp4', frames)\n","clear_output()\n","\n","from IPython.display import HTML\n","import io\n","import base64\n","video = io.open('/content/CLIP/video.mp4', 'r+b').read()\n","encoded = base64.b64encode(video)\n","play_html = ('<video alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /> </video>'.format(encoded.decode('ascii')))\n","HTML(data=play_html)"]},{"cell_type":"markdown","metadata":{"id":"glN_hbbLjxm-"},"source":["#### **이미지 및 영상 다운로드**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Di3LTW84O1i"},"outputs":[],"source":["files.download('/content/CLIP/result.png')\n","files.download('/content/CLIP/video.mp4')"]},{"cell_type":"markdown","metadata":{"id":"yv3C5MgZ_do1"},"source":["#### 다른 텍스트 데이터를 직접 넣어보고, 파라미터를 바꾸어가며 해보세요. "]},{"cell_type":"markdown","metadata":{"id":"Xnan4rZ6dYoG"},"source":["#### **TO-DO Codes - 4** ####"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJYGh6C6_b53"},"outputs":[],"source":["text =     # 원하는 텍스트 입력\n","translation = translator.translate(text)\n","prompt = translation.text\n","tx = clip.tokenize(prompt)\n","\n","resolution =         # 예시: 128, 256, 512\n","frames_frequency =   # 예시: 5, 10, 50, 100 \n","stop_iter =          # 예시: 10, 20, 50\n","sideX, sideY, channels = resolution, resolution, 3\n","\n","!rm -rf /content/CLIP/frames/*.*\n","model = Siren(in_features=2, out_features=3, hidden_features=256, \n","                  hidden_layers=16, outermost_linear=False).cuda()\n","optimizer = torch.optim.Adam(model.parameters(), .00001)\n","\n","for itt in range(stop_iter*frames_frequency):\n","  train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhVTj_RW_cw8"},"outputs":[],"source":["# 비디오로 출력\n","\n","frames = []\n","img = os.listdir(\"/content/CLIP/frames\")\n","img.sort()\n","for i in img:\n","  frames.append(imageio.imread(\"/content/CLIP/frames/\"+i))\n","frames = np.array(frames)\n","imageio.mimsave('/content/CLIP/video.mp4', frames)\n","clear_output()\n","\n","from IPython.display import HTML\n","import io\n","import base64\n","video = io.open('/content/CLIP/video.mp4', 'r+b').read()\n","encoded = base64.b64encode(video) \n","play_html = ('<video alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /> </video>'.format(encoded.decode('ascii')))\n","HTML(data=play_html) "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"(기본) 5번 과제 - Pre-trained multi-modal model applications with CLIP (문제).ipynb","provenance":[{"file_id":"1tMR4L7JxAkh47zOD182g1RTX822BRII_","timestamp":1647408996404}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
